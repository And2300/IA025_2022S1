{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula7_Exercício_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/And2300/IA025_2022S1/blob/main/Aula7_Exerc%C3%ADcio_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Andersson Romero'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "9234b4fa-d514-4386-b039-b143501e08c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Andersson Romero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3twP0YJC4jmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc53bb6-f614-47a2-fab8-cd582c725576"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f44423-2fa2-4dc5-9c6f-2af2616e88c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 18 17:18:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    13W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19056239-2003-457b-f57f-8717cb7bee92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, context_size: int): #from Edmar Rodriguez\n",
        "        # Escreva seu código aqui  \n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_size = context_size\n",
        "\n",
        "        self.items = []\n",
        "        for text in self.texts:\n",
        "            text = tokenize(text, self.tokenizer)\n",
        "            for cidx in range(0, len(text)-self.context_size):\n",
        "                x = text[cidx : cidx+self.context_size]\n",
        "                x = torch.tensor(x)\n",
        "                y = text[cidx + self.context_size]\n",
        "                y = torch.tensor(y)\n",
        "                self.items.append((x,y))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        return self.items[idx]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste se sua implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125],\n",
        "     [ 1660,  5971,   785],\n",
        "     [ 5971,   785,   125],\n",
        "     [  785,   125,  1847],\n",
        "     [  125,  1847, 13779]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6badb2-e30d-4c26-8751-9a42cb94d2d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ],
      "metadata": {
        "id": "vGlN1WqrXPA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f070e5f7-bb27-4b8e-a12c-5adf6b825474"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample_brwac.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "print('Truncating for debugging purposes.')\n",
        "texts = texts[:500]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e1b35b-ef8b-4e42-8713-801643e99169"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncating for debugging purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7799b97-d5ba-408c-9359-5281d41fad54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 406905\n",
            "valid examples: 135562\n",
            "test examples: 136690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(self.embedding_dim * self.context_size, self.hidden_size)  ##self.linear2 = nn.Linear(hidden_size, hidden_size*2)\n",
        "        self.linear2 = nn.Linear(self.hidden_size, self.vocab_size, bias = False)  #camada de salida\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        out = self.embedding(inputs).view(-1, self.context_size*self.embedding_dim)\n",
        "        out = self.linear1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        #out = self.relu(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f5c4c5-27da-45fb-9e55-94f1b5bb5b65"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 29794])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08f0c11-6809-4765-d10a-9f33606a14ff"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5794304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    # Escreva seu código aqui.\n",
        "    loss = nn.CrossEntropyLoss()(logits, target)\n",
        "    return torch.exp(loss).sum()\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be5b81b-1be7-4980-c2d9-a0a7f794eb70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              31005\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20121ae-3e7a-4fe3-fb66-c4d619227849"
      },
      "source": [
        "max_examples = 100_000_000\n",
        "eval_every_steps = 5000\n",
        "lr = 3e-5\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=256,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 31404.72, valid ppl: 30466.99\n",
            "5000 steps; 320000 examples so far; train ppl: 2840.94, valid ppl: 1939.88\n",
            "10000 steps; 640000 examples so far; train ppl: 1319.25, valid ppl: 1783.05\n",
            "15000 steps; 960000 examples so far; train ppl: 1062.31, valid ppl: 1792.37\n",
            "20000 steps; 1280000 examples so far; train ppl: 908.95, valid ppl: 1830.38\n",
            "25000 steps; 1600000 examples so far; train ppl: 777.16, valid ppl: 1844.99\n",
            "30000 steps; 1920000 examples so far; train ppl: 662.07, valid ppl: 1879.71\n",
            "35000 steps; 2240000 examples so far; train ppl: 570.81, valid ppl: 1925.32\n",
            "40000 steps; 2560000 examples so far; train ppl: 491.17, valid ppl: 2001.25\n",
            "45000 steps; 2880000 examples so far; train ppl: 423.00, valid ppl: 2085.55\n",
            "50000 steps; 3200000 examples so far; train ppl: 356.92, valid ppl: 2185.44\n",
            "55000 steps; 3520000 examples so far; train ppl: 302.36, valid ppl: 2364.04\n",
            "60000 steps; 3840000 examples so far; train ppl: 258.51, valid ppl: 2581.82\n",
            "65000 steps; 4160000 examples so far; train ppl: 217.83, valid ppl: 2877.88\n",
            "70000 steps; 4480000 examples so far; train ppl: 186.34, valid ppl: 3118.45\n",
            "75000 steps; 4800000 examples so far; train ppl: 153.23, valid ppl: 3529.50\n",
            "80000 steps; 5120000 examples so far; train ppl: 130.36, valid ppl: 4003.47\n",
            "85000 steps; 5440000 examples so far; train ppl: 112.61, valid ppl: 4519.84\n",
            "90000 steps; 5760000 examples so far; train ppl: 98.10, valid ppl: 5058.49\n",
            "95000 steps; 6080000 examples so far; train ppl: 85.80, valid ppl: 5542.00\n",
            "100000 steps; 6400000 examples so far; train ppl: 75.14, valid ppl: 6248.80\n",
            "105000 steps; 6720000 examples so far; train ppl: 67.80, valid ppl: 6942.45\n",
            "110000 steps; 7040000 examples so far; train ppl: 62.38, valid ppl: 7703.90\n",
            "115000 steps; 7360000 examples so far; train ppl: 57.87, valid ppl: 8331.21\n",
            "120000 steps; 7680000 examples so far; train ppl: 52.51, valid ppl: 9007.02\n",
            "125000 steps; 8000000 examples so far; train ppl: 48.57, valid ppl: 9973.80\n",
            "130000 steps; 8320000 examples so far; train ppl: 45.63, valid ppl: 11004.59\n",
            "135000 steps; 8640000 examples so far; train ppl: 43.21, valid ppl: 11902.96\n",
            "140000 steps; 8960000 examples so far; train ppl: 41.01, valid ppl: 12563.20\n",
            "145000 steps; 9280000 examples so far; train ppl: 37.81, valid ppl: 13793.49\n",
            "150000 steps; 9600000 examples so far; train ppl: 36.05, valid ppl: 15134.59\n",
            "155000 steps; 9920000 examples so far; train ppl: 34.31, valid ppl: 16499.28\n",
            "160000 steps; 10240000 examples so far; train ppl: 33.04, valid ppl: 17762.92\n",
            "165000 steps; 10560000 examples so far; train ppl: 31.58, valid ppl: 18659.52\n",
            "170000 steps; 10880000 examples so far; train ppl: 29.58, valid ppl: 20398.06\n",
            "175000 steps; 11200000 examples so far; train ppl: 28.47, valid ppl: 22253.35\n",
            "180000 steps; 11520000 examples so far; train ppl: 27.66, valid ppl: 23983.21\n",
            "185000 steps; 11840000 examples so far; train ppl: 26.69, valid ppl: 25594.56\n",
            "190000 steps; 12160000 examples so far; train ppl: 25.43, valid ppl: 27111.87\n",
            "195000 steps; 12480000 examples so far; train ppl: 24.30, valid ppl: 29562.81\n",
            "200000 steps; 12800000 examples so far; train ppl: 23.58, valid ppl: 31814.56\n",
            "205000 steps; 13120000 examples so far; train ppl: 22.99, valid ppl: 34384.44\n",
            "210000 steps; 13440000 examples so far; train ppl: 22.35, valid ppl: 36031.67\n",
            "215000 steps; 13760000 examples so far; train ppl: 21.25, valid ppl: 38440.43\n",
            "220000 steps; 14080000 examples so far; train ppl: 20.69, valid ppl: 41760.76\n",
            "225000 steps; 14400000 examples so far; train ppl: 20.08, valid ppl: 44676.31\n",
            "230000 steps; 14720000 examples so far; train ppl: 19.54, valid ppl: 47864.66\n",
            "235000 steps; 15040000 examples so far; train ppl: 19.14, valid ppl: 49999.34\n",
            "240000 steps; 15360000 examples so far; train ppl: 18.22, valid ppl: 53609.20\n",
            "245000 steps; 15680000 examples so far; train ppl: 17.88, valid ppl: 57866.15\n",
            "250000 steps; 16000000 examples so far; train ppl: 17.37, valid ppl: 61890.62\n",
            "255000 steps; 16320000 examples so far; train ppl: 17.20, valid ppl: 65188.03\n",
            "260000 steps; 16640000 examples so far; train ppl: 16.56, valid ppl: 68515.99\n",
            "265000 steps; 16960000 examples so far; train ppl: 16.07, valid ppl: 73438.68\n",
            "270000 steps; 17280000 examples so far; train ppl: 15.60, valid ppl: 78594.90\n",
            "275000 steps; 17600000 examples so far; train ppl: 15.51, valid ppl: 83507.98\n",
            "280000 steps; 17920000 examples so far; train ppl: 15.15, valid ppl: 86915.14\n",
            "285000 steps; 18240000 examples so far; train ppl: 14.58, valid ppl: 91983.50\n",
            "290000 steps; 18560000 examples so far; train ppl: 14.25, valid ppl: 98111.57\n",
            "295000 steps; 18880000 examples so far; train ppl: 13.96, valid ppl: 105022.82\n",
            "300000 steps; 19200000 examples so far; train ppl: 13.85, valid ppl: 110773.90\n",
            "305000 steps; 19520000 examples so far; train ppl: 13.59, valid ppl: 113902.91\n",
            "310000 steps; 19840000 examples so far; train ppl: 13.06, valid ppl: 122120.16\n",
            "315000 steps; 20160000 examples so far; train ppl: 12.82, valid ppl: 130158.97\n",
            "320000 steps; 20480000 examples so far; train ppl: 12.61, valid ppl: 137974.68\n",
            "325000 steps; 20800000 examples so far; train ppl: 12.52, valid ppl: 145536.12\n",
            "330000 steps; 21120000 examples so far; train ppl: 12.22, valid ppl: 149298.46\n",
            "335000 steps; 21440000 examples so far; train ppl: 11.81, valid ppl: 160844.56\n",
            "340000 steps; 21760000 examples so far; train ppl: 11.64, valid ppl: 170623.64\n",
            "345000 steps; 22080000 examples so far; train ppl: 11.57, valid ppl: 181288.80\n",
            "350000 steps; 22400000 examples so far; train ppl: 11.37, valid ppl: 186780.92\n",
            "355000 steps; 22720000 examples so far; train ppl: 11.08, valid ppl: 195952.76\n",
            "360000 steps; 23040000 examples so far; train ppl: 10.82, valid ppl: 209669.01\n",
            "365000 steps; 23360000 examples so far; train ppl: 10.70, valid ppl: 221055.25\n",
            "370000 steps; 23680000 examples so far; train ppl: 10.58, valid ppl: 234231.53\n",
            "375000 steps; 24000000 examples so far; train ppl: 10.45, valid ppl: 240046.75\n",
            "380000 steps; 24320000 examples so far; train ppl: 10.06, valid ppl: 256491.35\n",
            "385000 steps; 24640000 examples so far; train ppl: 10.01, valid ppl: 274985.85\n",
            "390000 steps; 24960000 examples so far; train ppl: 9.86, valid ppl: 288820.17\n",
            "395000 steps; 25280000 examples so far; train ppl: 9.73, valid ppl: 302824.69\n",
            "400000 steps; 25600000 examples so far; train ppl: 9.62, valid ppl: 314193.42\n",
            "405000 steps; 25920000 examples so far; train ppl: 9.33, valid ppl: 333755.56\n",
            "410000 steps; 26240000 examples so far; train ppl: 9.23, valid ppl: 357424.76\n",
            "415000 steps; 26560000 examples so far; train ppl: 9.18, valid ppl: 376818.88\n",
            "420000 steps; 26880000 examples so far; train ppl: 9.07, valid ppl: 392608.66\n",
            "425000 steps; 27200000 examples so far; train ppl: 8.86, valid ppl: 412025.03\n",
            "430000 steps; 27520000 examples so far; train ppl: 8.65, valid ppl: 436141.68\n",
            "435000 steps; 27840000 examples so far; train ppl: 8.64, valid ppl: 464690.93\n",
            "440000 steps; 28160000 examples so far; train ppl: 8.53, valid ppl: 491990.57\n",
            "445000 steps; 28480000 examples so far; train ppl: 8.46, valid ppl: 504664.40\n",
            "450000 steps; 28800000 examples so far; train ppl: 8.21, valid ppl: 537227.98\n",
            "455000 steps; 29120000 examples so far; train ppl: 8.06, valid ppl: 572589.88\n",
            "460000 steps; 29440000 examples so far; train ppl: 8.07, valid ppl: 610543.13\n",
            "465000 steps; 29760000 examples so far; train ppl: 8.01, valid ppl: 635921.70\n",
            "470000 steps; 30080000 examples so far; train ppl: 7.89, valid ppl: 656618.23\n",
            "475000 steps; 30400000 examples so far; train ppl: 7.68, valid ppl: 702869.41\n",
            "480000 steps; 30720000 examples so far; train ppl: 7.61, valid ppl: 743272.48\n",
            "485000 steps; 31040000 examples so far; train ppl: 7.58, valid ppl: 790924.82\n",
            "490000 steps; 31360000 examples so far; train ppl: 7.51, valid ppl: 822947.78\n",
            "495000 steps; 31680000 examples so far; train ppl: 7.33, valid ppl: 857811.36\n",
            "500000 steps; 32000000 examples so far; train ppl: 7.27, valid ppl: 912334.51\n",
            "505000 steps; 32320000 examples so far; train ppl: 7.16, valid ppl: 964832.00\n",
            "510000 steps; 32640000 examples so far; train ppl: 7.12, valid ppl: 1023047.09\n",
            "515000 steps; 32960000 examples so far; train ppl: 7.10, valid ppl: 1051731.20\n",
            "520000 steps; 33280000 examples so far; train ppl: 6.88, valid ppl: 1114573.63\n",
            "525000 steps; 33600000 examples so far; train ppl: 6.82, valid ppl: 1185000.51\n",
            "530000 steps; 33920000 examples so far; train ppl: 6.77, valid ppl: 1251823.46\n",
            "535000 steps; 34240000 examples so far; train ppl: 6.76, valid ppl: 1318740.89\n",
            "540000 steps; 34560000 examples so far; train ppl: 6.67, valid ppl: 1362426.98\n",
            "545000 steps; 34880000 examples so far; train ppl: 6.51, valid ppl: 1446743.56\n",
            "550000 steps; 35200000 examples so far; train ppl: 6.47, valid ppl: 1548035.60\n",
            "555000 steps; 35520000 examples so far; train ppl: 6.42, valid ppl: 1627051.88\n",
            "560000 steps; 35840000 examples so far; train ppl: 6.40, valid ppl: 1702302.33\n",
            "565000 steps; 36160000 examples so far; train ppl: 6.28, valid ppl: 1768129.58\n",
            "570000 steps; 36480000 examples so far; train ppl: 6.17, valid ppl: 1885225.01\n",
            "575000 steps; 36800000 examples so far; train ppl: 6.14, valid ppl: 2007669.05\n",
            "580000 steps; 37120000 examples so far; train ppl: 6.11, valid ppl: 2108467.55\n",
            "585000 steps; 37440000 examples so far; train ppl: 6.08, valid ppl: 2175433.23\n",
            "590000 steps; 37760000 examples so far; train ppl: 5.92, valid ppl: 2285595.93\n",
            "595000 steps; 38080000 examples so far; train ppl: 5.88, valid ppl: 2452458.57\n",
            "600000 steps; 38400000 examples so far; train ppl: 5.83, valid ppl: 2584197.60\n",
            "605000 steps; 38720000 examples so far; train ppl: 5.83, valid ppl: 2721773.94\n",
            "610000 steps; 39040000 examples so far; train ppl: 5.77, valid ppl: 2817082.38\n",
            "615000 steps; 39360000 examples so far; train ppl: 5.62, valid ppl: 2989014.04\n",
            "620000 steps; 39680000 examples so far; train ppl: 5.60, valid ppl: 3163087.22\n",
            "625000 steps; 40000000 examples so far; train ppl: 5.59, valid ppl: 3340245.08\n",
            "630000 steps; 40320000 examples so far; train ppl: 5.53, valid ppl: 3543948.93\n",
            "635000 steps; 40640000 examples so far; train ppl: 5.47, valid ppl: 3659389.32\n",
            "640000 steps; 40960000 examples so far; train ppl: 5.38, valid ppl: 3854995.87\n",
            "645000 steps; 41280000 examples so far; train ppl: 5.35, valid ppl: 4097105.26\n",
            "650000 steps; 41600000 examples so far; train ppl: 5.36, valid ppl: 4354345.14\n",
            "655000 steps; 41920000 examples so far; train ppl: 5.29, valid ppl: 4465813.50\n",
            "660000 steps; 42240000 examples so far; train ppl: 5.19, valid ppl: 4706032.02\n",
            "665000 steps; 42560000 examples so far; train ppl: 5.13, valid ppl: 5020609.51\n",
            "670000 steps; 42880000 examples so far; train ppl: 5.13, valid ppl: 5359449.93\n",
            "675000 steps; 43200000 examples so far; train ppl: 5.10, valid ppl: 5593293.25\n",
            "680000 steps; 43520000 examples so far; train ppl: 5.07, valid ppl: 5781547.90\n",
            "685000 steps; 43840000 examples so far; train ppl: 4.93, valid ppl: 6129621.36\n",
            "690000 steps; 44160000 examples so far; train ppl: 4.94, valid ppl: 6508429.21\n",
            "695000 steps; 44480000 examples so far; train ppl: 4.91, valid ppl: 6862659.97\n",
            "700000 steps; 44800000 examples so far; train ppl: 4.89, valid ppl: 7243353.50\n",
            "705000 steps; 45120000 examples so far; train ppl: 4.83, valid ppl: 7491762.72\n",
            "710000 steps; 45440000 examples so far; train ppl: 4.75, valid ppl: 8002684.83\n",
            "715000 steps; 45760000 examples so far; train ppl: 4.74, valid ppl: 8405107.00\n",
            "720000 steps; 46080000 examples so far; train ppl: 4.71, valid ppl: 8927077.37\n",
            "725000 steps; 46400000 examples so far; train ppl: 4.70, valid ppl: 9297081.82\n",
            "730000 steps; 46720000 examples so far; train ppl: 4.60, valid ppl: 9733682.20\n",
            "735000 steps; 47040000 examples so far; train ppl: 4.58, valid ppl: 10335671.72\n",
            "740000 steps; 47360000 examples so far; train ppl: 4.54, valid ppl: 10989319.82\n",
            "745000 steps; 47680000 examples so far; train ppl: 4.55, valid ppl: 11516417.79\n",
            "750000 steps; 48000000 examples so far; train ppl: 4.50, valid ppl: 11977015.97\n",
            "755000 steps; 48320000 examples so far; train ppl: 4.41, valid ppl: 12699434.15\n",
            "760000 steps; 48640000 examples so far; train ppl: 4.39, valid ppl: 13411645.47\n",
            "765000 steps; 48960000 examples so far; train ppl: 4.37, valid ppl: 14233885.78\n",
            "770000 steps; 49280000 examples so far; train ppl: 4.36, valid ppl: 14885710.03\n",
            "775000 steps; 49600000 examples so far; train ppl: 4.33, valid ppl: 15523911.51\n",
            "780000 steps; 49920000 examples so far; train ppl: 4.25, valid ppl: 16528883.39\n",
            "785000 steps; 50240000 examples so far; train ppl: 4.24, valid ppl: 17536558.01\n",
            "790000 steps; 50560000 examples so far; train ppl: 4.21, valid ppl: 18560124.91\n",
            "795000 steps; 50880000 examples so far; train ppl: 4.20, valid ppl: 19398483.83\n",
            "800000 steps; 51200000 examples so far; train ppl: 4.14, valid ppl: 20059283.01\n",
            "805000 steps; 51520000 examples so far; train ppl: 4.11, valid ppl: 21364815.00\n",
            "810000 steps; 51840000 examples so far; train ppl: 4.06, valid ppl: 22819410.80\n",
            "815000 steps; 52160000 examples so far; train ppl: 4.09, valid ppl: 23955778.95\n",
            "820000 steps; 52480000 examples so far; train ppl: 4.05, valid ppl: 24738678.78\n",
            "825000 steps; 52800000 examples so far; train ppl: 3.97, valid ppl: 26389594.03\n",
            "830000 steps; 53120000 examples so far; train ppl: 3.96, valid ppl: 28132046.16\n",
            "835000 steps; 53440000 examples so far; train ppl: 3.94, valid ppl: 29704586.77\n",
            "840000 steps; 53760000 examples so far; train ppl: 3.93, valid ppl: 30990183.71\n",
            "845000 steps; 54080000 examples so far; train ppl: 3.90, valid ppl: 32310738.72\n",
            "850000 steps; 54400000 examples so far; train ppl: 3.83, valid ppl: 34529592.29\n",
            "855000 steps; 54720000 examples so far; train ppl: 3.82, valid ppl: 36625601.14\n",
            "860000 steps; 55040000 examples so far; train ppl: 3.81, valid ppl: 38631835.38\n",
            "865000 steps; 55360000 examples so far; train ppl: 3.81, valid ppl: 40471019.00\n",
            "870000 steps; 55680000 examples so far; train ppl: 3.74, valid ppl: 42083277.02\n",
            "875000 steps; 56000000 examples so far; train ppl: 3.73, valid ppl: 44663331.95\n",
            "880000 steps; 56320000 examples so far; train ppl: 3.68, valid ppl: 47961848.41\n",
            "885000 steps; 56640000 examples so far; train ppl: 3.69, valid ppl: 50456684.85\n",
            "890000 steps; 56960000 examples so far; train ppl: 3.68, valid ppl: 52214104.30\n",
            "895000 steps; 57280000 examples so far; train ppl: 3.60, valid ppl: 55094875.45\n",
            "900000 steps; 57600000 examples so far; train ppl: 3.60, valid ppl: 58701579.96\n",
            "905000 steps; 57920000 examples so far; train ppl: 3.57, valid ppl: 62636321.30\n",
            "910000 steps; 58240000 examples so far; train ppl: 3.58, valid ppl: 65294534.46\n",
            "915000 steps; 58560000 examples so far; train ppl: 3.55, valid ppl: 68038003.72\n",
            "920000 steps; 58880000 examples so far; train ppl: 3.49, valid ppl: 72508486.14\n",
            "925000 steps; 59200000 examples so far; train ppl: 3.49, valid ppl: 77027968.85\n",
            "930000 steps; 59520000 examples so far; train ppl: 3.47, valid ppl: 81708522.77\n",
            "935000 steps; 59840000 examples so far; train ppl: 3.47, valid ppl: 85241440.43\n",
            "940000 steps; 60160000 examples so far; train ppl: 3.42, valid ppl: 88928482.62\n",
            "945000 steps; 60480000 examples so far; train ppl: 3.38, valid ppl: 94851792.62\n",
            "950000 steps; 60800000 examples so far; train ppl: 3.38, valid ppl: 100772467.01\n",
            "955000 steps; 61120000 examples so far; train ppl: 3.37, valid ppl: 106434810.69\n",
            "960000 steps; 61440000 examples so far; train ppl: 3.36, valid ppl: 110271932.14\n",
            "965000 steps; 61760000 examples so far; train ppl: 3.29, valid ppl: 117345970.60\n",
            "970000 steps; 62080000 examples so far; train ppl: 3.29, valid ppl: 125667940.84\n",
            "975000 steps; 62400000 examples so far; train ppl: 3.28, valid ppl: 132571611.17\n",
            "980000 steps; 62720000 examples so far; train ppl: 3.27, valid ppl: 139295462.67\n",
            "985000 steps; 63040000 examples so far; train ppl: 3.25, valid ppl: 144274476.89\n",
            "990000 steps; 63360000 examples so far; train ppl: 3.20, valid ppl: 153334063.09\n",
            "995000 steps; 63680000 examples so far; train ppl: 3.18, valid ppl: 162274382.49\n",
            "1000000 steps; 64000000 examples so far; train ppl: 3.19, valid ppl: 171944803.58\n",
            "1005000 steps; 64320000 examples so far; train ppl: 3.17, valid ppl: 182419008.85\n",
            "1010000 steps; 64640000 examples so far; train ppl: 3.14, valid ppl: 188113113.00\n",
            "1015000 steps; 64960000 examples so far; train ppl: 3.10, valid ppl: 201383872.74\n",
            "1020000 steps; 65280000 examples so far; train ppl: 3.11, valid ppl: 215058156.93\n",
            "1025000 steps; 65600000 examples so far; train ppl: 3.09, valid ppl: 227123344.15\n",
            "1030000 steps; 65920000 examples so far; train ppl: 3.09, valid ppl: 236963389.93\n",
            "1035000 steps; 66240000 examples so far; train ppl: 3.04, valid ppl: 250543434.26\n",
            "1040000 steps; 66560000 examples so far; train ppl: 3.02, valid ppl: 265404195.76\n",
            "1045000 steps; 66880000 examples so far; train ppl: 3.01, valid ppl: 281442870.21\n",
            "1050000 steps; 67200000 examples so far; train ppl: 3.02, valid ppl: 299299824.51\n",
            "1055000 steps; 67520000 examples so far; train ppl: 2.99, valid ppl: 309960511.13\n",
            "1060000 steps; 67840000 examples so far; train ppl: 2.95, valid ppl: 330375026.93\n",
            "1065000 steps; 68160000 examples so far; train ppl: 2.93, valid ppl: 349468134.53\n",
            "1070000 steps; 68480000 examples so far; train ppl: 2.94, valid ppl: 371300141.51\n",
            "1075000 steps; 68800000 examples so far; train ppl: 2.93, valid ppl: 392281777.91\n",
            "1080000 steps; 69120000 examples so far; train ppl: 2.90, valid ppl: 410327386.97\n",
            "1085000 steps; 69440000 examples so far; train ppl: 2.87, valid ppl: 434040824.51\n",
            "1090000 steps; 69760000 examples so far; train ppl: 2.86, valid ppl: 460865931.34\n",
            "1095000 steps; 70080000 examples so far; train ppl: 2.85, valid ppl: 490601338.81\n",
            "1100000 steps; 70400000 examples so far; train ppl: 2.86, valid ppl: 510347709.84\n",
            "1105000 steps; 70720000 examples so far; train ppl: 2.81, valid ppl: 537135263.41\n",
            "1110000 steps; 71040000 examples so far; train ppl: 2.80, valid ppl: 572014974.07\n",
            "1115000 steps; 71360000 examples so far; train ppl: 2.79, valid ppl: 604889250.36\n",
            "1120000 steps; 71680000 examples so far; train ppl: 2.78, valid ppl: 645547443.53\n",
            "1125000 steps; 72000000 examples so far; train ppl: 2.78, valid ppl: 666908181.16\n",
            "1130000 steps; 72320000 examples so far; train ppl: 2.73, valid ppl: 713201306.95\n",
            "1135000 steps; 72640000 examples so far; train ppl: 2.72, valid ppl: 758344660.35\n",
            "1140000 steps; 72960000 examples so far; train ppl: 2.71, valid ppl: 805857640.30\n",
            "1145000 steps; 73280000 examples so far; train ppl: 2.72, valid ppl: 842139396.48\n",
            "1150000 steps; 73600000 examples so far; train ppl: 2.69, valid ppl: 883904077.73\n",
            "1155000 steps; 73920000 examples so far; train ppl: 2.65, valid ppl: 944855696.45\n",
            "1160000 steps; 74240000 examples so far; train ppl: 2.66, valid ppl: 998871086.95\n",
            "1165000 steps; 74560000 examples so far; train ppl: 2.66, valid ppl: 1063420874.54\n",
            "1170000 steps; 74880000 examples so far; train ppl: 2.64, valid ppl: 1111636375.85\n",
            "1175000 steps; 75200000 examples so far; train ppl: 2.61, valid ppl: 1171598384.61\n",
            "1180000 steps; 75520000 examples so far; train ppl: 2.59, valid ppl: 1248654180.44\n",
            "1185000 steps; 75840000 examples so far; train ppl: 2.60, valid ppl: 1328814411.25\n",
            "1190000 steps; 76160000 examples so far; train ppl: 2.59, valid ppl: 1405042356.92\n",
            "1195000 steps; 76480000 examples so far; train ppl: 2.57, valid ppl: 1454764865.42\n",
            "1200000 steps; 76800000 examples so far; train ppl: 2.54, valid ppl: 1556339997.53\n",
            "1205000 steps; 77120000 examples so far; train ppl: 2.53, valid ppl: 1654752372.29\n",
            "1210000 steps; 77440000 examples so far; train ppl: 2.53, valid ppl: 1759839760.07\n",
            "1215000 steps; 77760000 examples so far; train ppl: 2.52, valid ppl: 1850148002.78\n",
            "1220000 steps; 78080000 examples so far; train ppl: 2.51, valid ppl: 1944848361.59\n",
            "1225000 steps; 78400000 examples so far; train ppl: 2.48, valid ppl: 2052716808.69\n",
            "1230000 steps; 78720000 examples so far; train ppl: 2.47, valid ppl: 2191934223.70\n",
            "1235000 steps; 79040000 examples so far; train ppl: 2.47, valid ppl: 2313440621.22\n",
            "1240000 steps; 79360000 examples so far; train ppl: 2.47, valid ppl: 2449057027.34\n",
            "1245000 steps; 79680000 examples so far; train ppl: 2.43, valid ppl: 2567076675.85\n",
            "1250000 steps; 80000000 examples so far; train ppl: 2.42, valid ppl: 2710818544.49\n",
            "1255000 steps; 80320000 examples so far; train ppl: 2.41, valid ppl: 2897405035.00\n",
            "1260000 steps; 80640000 examples so far; train ppl: 2.42, valid ppl: 3077008703.84\n",
            "1265000 steps; 80960000 examples so far; train ppl: 2.41, valid ppl: 3211894385.26\n",
            "1270000 steps; 81280000 examples so far; train ppl: 2.37, valid ppl: 3426483098.48\n",
            "1275000 steps; 81600000 examples so far; train ppl: 2.36, valid ppl: 3648814231.72\n",
            "1280000 steps; 81920000 examples so far; train ppl: 2.37, valid ppl: 3869572483.91\n",
            "1285000 steps; 82240000 examples so far; train ppl: 2.35, valid ppl: 4076109281.49\n",
            "1290000 steps; 82560000 examples so far; train ppl: 2.34, valid ppl: 4287761621.49\n",
            "1295000 steps; 82880000 examples so far; train ppl: 2.31, valid ppl: 4563823273.35\n",
            "1300000 steps; 83200000 examples so far; train ppl: 2.31, valid ppl: 4855863106.35\n",
            "1305000 steps; 83520000 examples so far; train ppl: 2.31, valid ppl: 5191519154.17\n",
            "1310000 steps; 83840000 examples so far; train ppl: 2.30, valid ppl: 5437832040.88\n",
            "1315000 steps; 84160000 examples so far; train ppl: 2.28, valid ppl: 5673909863.13\n",
            "1320000 steps; 84480000 examples so far; train ppl: 2.26, valid ppl: 6081298613.41\n",
            "1325000 steps; 84800000 examples so far; train ppl: 2.26, valid ppl: 6494510382.26\n",
            "1330000 steps; 85120000 examples so far; train ppl: 2.26, valid ppl: 6830942870.69\n",
            "1335000 steps; 85440000 examples so far; train ppl: 2.25, valid ppl: 7162067020.01\n",
            "1340000 steps; 85760000 examples so far; train ppl: 2.22, valid ppl: 7650416630.64\n",
            "1345000 steps; 86080000 examples so far; train ppl: 2.21, valid ppl: 8118756869.66\n",
            "1350000 steps; 86400000 examples so far; train ppl: 2.22, valid ppl: 8610144668.78\n",
            "1355000 steps; 86720000 examples so far; train ppl: 2.20, valid ppl: 9166208663.81\n",
            "1360000 steps; 87040000 examples so far; train ppl: 2.20, valid ppl: 9486515281.57\n",
            "1365000 steps; 87360000 examples so far; train ppl: 2.17, valid ppl: 10192873705.21\n",
            "1370000 steps; 87680000 examples so far; train ppl: 2.17, valid ppl: 10798907812.78\n",
            "1375000 steps; 88000000 examples so far; train ppl: 2.16, valid ppl: 11520002424.33\n",
            "1380000 steps; 88320000 examples so far; train ppl: 2.16, valid ppl: 12133629933.23\n",
            "1385000 steps; 88640000 examples so far; train ppl: 2.15, valid ppl: 12703242920.43\n",
            "1390000 steps; 88960000 examples so far; train ppl: 2.12, valid ppl: 13644095492.12\n",
            "1395000 steps; 89280000 examples so far; train ppl: 2.12, valid ppl: 14495144581.89\n",
            "1400000 steps; 89600000 examples so far; train ppl: 2.12, valid ppl: 15385404564.09\n",
            "1405000 steps; 89920000 examples so far; train ppl: 2.12, valid ppl: 16047918110.78\n",
            "1410000 steps; 90240000 examples so far; train ppl: 2.09, valid ppl: 17134692107.80\n",
            "1415000 steps; 90560000 examples so far; train ppl: 2.08, valid ppl: 18315895672.27\n",
            "1420000 steps; 90880000 examples so far; train ppl: 2.08, valid ppl: 19446621909.51\n",
            "1425000 steps; 91200000 examples so far; train ppl: 2.08, valid ppl: 20456216010.50\n",
            "1430000 steps; 91520000 examples so far; train ppl: 2.06, valid ppl: 21442593541.56\n",
            "1435000 steps; 91840000 examples so far; train ppl: 2.04, valid ppl: 22752795757.80\n",
            "1440000 steps; 92160000 examples so far; train ppl: 2.03, valid ppl: 24483194784.53\n",
            "1445000 steps; 92480000 examples so far; train ppl: 2.04, valid ppl: 26084680338.41\n",
            "1450000 steps; 92800000 examples so far; train ppl: 2.03, valid ppl: 27508401445.58\n",
            "1455000 steps; 93120000 examples so far; train ppl: 2.02, valid ppl: 28981327903.43\n",
            "1460000 steps; 93440000 examples so far; train ppl: 2.00, valid ppl: 30659722579.82\n",
            "1465000 steps; 93760000 examples so far; train ppl: 2.00, valid ppl: 32780120641.80\n",
            "1470000 steps; 94080000 examples so far; train ppl: 1.99, valid ppl: 34856035816.06\n",
            "1475000 steps; 94400000 examples so far; train ppl: 1.99, valid ppl: 36680031890.74\n",
            "1480000 steps; 94720000 examples so far; train ppl: 1.96, valid ppl: 38827151514.39\n",
            "1485000 steps; 95040000 examples so far; train ppl: 1.96, valid ppl: 41293828126.36\n",
            "1490000 steps; 95360000 examples so far; train ppl: 1.95, valid ppl: 44112729960.62\n",
            "1495000 steps; 95680000 examples so far; train ppl: 1.95, valid ppl: 46722098515.31\n",
            "1500000 steps; 96000000 examples so far; train ppl: 1.95, valid ppl: 49173458763.18\n",
            "1505000 steps; 96320000 examples so far; train ppl: 1.92, valid ppl: 52241452905.06\n",
            "1510000 steps; 96640000 examples so far; train ppl: 1.92, valid ppl: 55877083669.81\n",
            "1515000 steps; 96960000 examples so far; train ppl: 1.93, valid ppl: 59786956557.13\n",
            "1520000 steps; 97280000 examples so far; train ppl: 1.92, valid ppl: 62596379650.06\n",
            "1525000 steps; 97600000 examples so far; train ppl: 1.90, valid ppl: 66109999648.29\n",
            "1530000 steps; 97920000 examples so far; train ppl: 1.88, valid ppl: 70398091874.01\n",
            "1535000 steps; 98240000 examples so far; train ppl: 1.88, valid ppl: 74945260108.98\n",
            "1540000 steps; 98560000 examples so far; train ppl: 1.88, valid ppl: 79665324638.92\n",
            "1545000 steps; 98880000 examples so far; train ppl: 1.88, valid ppl: 84015931924.17\n",
            "1550000 steps; 99200000 examples so far; train ppl: 1.86, valid ppl: 89108954115.80\n",
            "1555000 steps; 99520000 examples so far; train ppl: 1.85, valid ppl: 94784693203.75\n",
            "1560000 steps; 99840000 examples so far; train ppl: 1.85, valid ppl: 101661085797.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efd31d0-9338-4704-b7bf-7cf90665575e"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 140025740329.32248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device)) #model(input_ids_truncated.to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d60be8-b8ef-4b98-a7e7-0c98cf2600cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de comer pizza pois me faz mal\n",
            "Eu gosto de comer pizza pois me faz maltra\n",
            "Eu gosto de comer pizza pois me faz maltrata\n",
            "Eu gosto de comer pizza pois me faz maltrata,\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu sou\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu soui\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu soui,\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu soui, me\n",
            "Eu gosto de comer pizza pois me faz maltrata, eu soui, me chu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uvWF7qaFzWCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}